{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab2039c",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7689cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from env.eehemt_env import EEHEMTEnv_Norm, tunable_params_config\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59ba34",
   "metadata": {},
   "source": [
    "## Define args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = (\n",
    "    \"/home/u5977862/DRL-on-parameter-extraction/data/S25E02A025WS_25C_GMVG.csv\"\n",
    ")\n",
    "va_file_path = \"/home/u5977862/DRL-on-parameter-extraction/eehemt/eehemt114_2.va\"\n",
    "test_modified = True\n",
    "n_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a8b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_learners: 4\n",
      "num_gpus_per_learner: 1.0\n"
     ]
    }
   ],
   "source": [
    "if th.cuda.device_count() == 4:\n",
    "    num_learners = 4\n",
    "    num_gpus_per_learner = 1.0\n",
    "elif th.cuda.device_count == 2:\n",
    "    num_learners = 2\n",
    "    num_gpus_per_learner = 1.0\n",
    "print(f\"num_learners: {num_learners}\\nnum_gpus_per_learner: {num_gpus_per_learner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68322240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.env.env_runner_group import EnvRunnerGroup\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "\n",
    "\n",
    "def run_and_plot_evaluation(\n",
    "    algorithm: Algorithm, eval_workers: EnvRunnerGroup\n",
    ") -> ResultDict:\n",
    "    \"\"\"\n",
    "    Custom evaluation function that runs one episode, plots the I-V curve,\n",
    "    and returns final metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running final evaluation and plotting I-V curve... ---\")\n",
    "\n",
    "    # 1. Get the local evaluation worker, its environment, and the trained policy.\n",
    "    local_worker = eval_workers.local_worker()\n",
    "    env = local_worker.env\n",
    "    policy = algorithm.get_policy()\n",
    "\n",
    "    # 2. Run a single, deterministic episode to find the best parameters.\n",
    "    obs, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        action, _, _ = policy.compute_single_action(observation=obs, explore=False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Final evaluation episode finished.\")\n",
    "    print(f\"Final RMSPE: {info['current_rmspe']:.6f}\")\n",
    "    print(\"Final Tunable Parameters:\")\n",
    "    final_tunable_params = {\n",
    "        k: info[\"current_params\"][k] for k in tunable_params_config.keys()\n",
    "    }\n",
    "    pprint.pprint(final_tunable_params)\n",
    "\n",
    "    # 3. Plot the I-V curve using the environment's final state.\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_path = os.path.join(output_dir, \"final_iv_curve.png\")\n",
    "\n",
    "    env.plot_iv_curve(\n",
    "        plot_initial=True, plot_modified=True, plot_current=True, save_path=save_path\n",
    "    )\n",
    "\n",
    "    # 4. Return a dictionary of final metrics.\n",
    "    return {\n",
    "        \"final_episode_reward\": total_reward,\n",
    "        \"final_rmspe\": info[\"current_rmspe\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79efcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        EEHEMTEnv_Norm,\n",
    "        env_config={\n",
    "            \"csv_file_path\": csv_file_path,\n",
    "            \"tunable_params_config\": tunable_params_config,\n",
    "            \"va_file_path\": va_file_path,\n",
    "            \"test_modified\": test_modified,\n",
    "        },\n",
    "    )\n",
    "    .env_runners(\n",
    "        observation_filter=\"MeanStdFilter\",  # Z-score norm better than L2 norm.\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000,\n",
    "        lr=0.0004,\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=num_learners,\n",
    "        num_gpus_per_learner=num_gpus_per_learner,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .evaluation(\n",
    "        # We only need one evaluation worker for plotting\n",
    "        evaluation_num_env_runners=1,\n",
    "        # We will call `evaluate()` manually, so no interval is needed.\n",
    "        evaluation_interval=None,\n",
    "        # Point to our custom function\n",
    "        custom_evaluation_function=run_and_plot_evaluation,\n",
    "        # Ensure evaluation is deterministic\n",
    "        evaluation_config={\"explore\": False},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc7ec7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23475b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 12:02:02,687\tWARNING algorithm_config.py:4921 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation doesn't occur automatically with each call to `Algorithm.train()`. Instead, you have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "[2025-08-06 12:02:02,733 E 54475 54475] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '70f14b6bee89ce8f000ae38001000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 12:02:02,766 E 54475 54475] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '950e7c1b8192240292f4443e01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 12:02:06,545\tWARNING algorithm_config.py:4921 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation doesn't occur automatically with each call to `Algorithm.train()`. Instead, you have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "[2025-08-06 12:02:06,580 E 54475 54475] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'cff6b5c2ac22db0f04ad6eec01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    }
   ],
   "source": [
    "algo = config.build_algo()\n",
    "\n",
    "# Run the training loop\n",
    "for i in range(n_iterations):\n",
    "    results = algo.train()\n",
    "    print(f\"--- Iteration: {i + 1}/{n_iterations} ---\")\n",
    "    print(f\"Episode Reward Mean: {results['episode_reward_mean']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training completed. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e774322",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efee778",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = algo.evaluate()\n",
    "print(\"\\n--- Custom evaluation results ---\")\n",
    "pprint.pprint(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved algo to /home/u5977862/DRL-on-parameter-extraction/result\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/home/u5977862/DRL-on-parameter-extraction/result/1\"\n",
    "checkpoint_dir = algo.save_to_path(checkpoint_dir)\n",
    "print(f\"\\nFinal algorithm checkpoint saved to: {checkpoint_dir}\")\n",
    "\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"\\n--- Script finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd261aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-on-parameter-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
